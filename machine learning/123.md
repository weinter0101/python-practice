# Marchine Learning

## 1. k-NN regression

```python
import numpy as np
import matplotlib.pyplot as plt

# create data
np.random.seed(20)
x = np.random.uniform(0, 10, 18)
y = np.random.uniform(0, 10, 18)

# create scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(x, y, c='blue', label='$Random\; Points$')
plt.title('$Scatter\; Plot\; with\; 18\; Random\; Points$')
plt.xlabel('$X$')
plt.ylabel('$Y$')
plt.legend()
plt.show()
```
![scatter](https://github.com/weinter0101/python-practice/blob/main/machine%20learning/figure/Figure1.1.png)


### k=n
- k=1
```python
k = 1
knnRegressor = KNeighborsRegressor(k)
knnRegressor.fit(x, y)

T = np.linspace(0, 10, 500)[:, np.newaxis] 
knnLine = knnRegressor.predict(T)

plt.figure(figsize=(10, 6))
plt.scatter(x, y, color='blue', label='$Data\; Points$')
plt.plot(T, knnLine, color='red', label=f'$k={k}\; Regression$')
plt.title('$k-NN\; Regression\; with\; k=3$')
plt.xlabel('$X$')
plt.ylabel('$Y$')
plt.legend()
plt.show()
```
![scatter](https://github.com/weinter0101/python-practice/blob/main/machine%20learning/figure/Figure1.2.png)

- k=2, 3, 5, 18
     [code](https://gist.github.com/7e0deab4e3c6ca9323e3c195fc77b71f.git)
![scatter](https://github.com/weinter0101/python-practice/blob/main/machine%20learning/figure/Figure1.3.png)

1. k值過小：overfitting, 無法對未見過的數據進行準確預測。
2. k值過大：underfitting, 由於過度平滑所以對訓練數據和新數據都不能進行有效的預測。

## 1. Linear Regression Model
- find a linear function to fit oberseve data
- Least Squares (LS) estimation:
     - Fitting criterion: sum of square distance between actual y and predicted y in the traning set
     - Find a function that minimizes this critersoin.
- general form of linear function:

$$
\begin{align*}
\mathbf{f}(\mathbf{x}; \boldsymbol{\beta}) &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k \quad (x_0 = 1) \\
&= \sum_{j=0}^{k} \beta_j x_j \\
&= \mathbf{x}^{\top} \boldsymbol{\beta},
\end{align*}
$$



- define a loss function(fitting criterion):

$$
square \quad loss: \( l(y, \hat{y}) = (y - \hat{y})^2 \)
$$

- example: advertising data
```python
import pandas as pd
import os
import matplotlib.pyplot as plt
import statsmodels.formula.api as smf #OLS
import seaborn as sns

path = r'C:\Users\chewei\Documents\python-practice\machine learning\data'
name = 'Advertising.csv'

advertising = pd.read_csv(os.path.join(path, name))
advertising.info()
advertising.head(3)

dependentVariable = ['TV', 'radio', 'newspaper']

for variables in dependentVariable:
    est = smf.ols(f"sales ~ {variables}", advertising).fit()
    print(est.summary())
    
est = smf.ols('sales ~ TV + radio + newspaper', advertising).fit()
print(est.summary())
print(advertising.corr())

est = smf.ols('sales ~ TV + radio + TV * radio', advertising).fit()
print(est.summary())

#%% figure
plt.figure(figsize=(18, 6))

for i, variable in enumerate(dependentVariable, 1):
    plt.subplot(1, 3, i) 
    sns.regplot(x=variable, y='sales', data=advertising)
    plt.title(f'Sales to {variable}')
    plt.xlabel(variable)
    plt.ylabel('Sales')

plt.tight_layout()
plt.show()
```
![scatter](https://github.com/weinter0101/python-practice/blob/main/machine%20learning/figure/Figure1.4.png)




## Gradient Descent

- 無法使用 Least Square Method 求解 $\beta$
     - the data is too large to compute the inverse for X'X
     - the colsed form solution is not exist
- 
$$
\begin{align*}
- \beta^{(1)} = \beta^{(0)} + \eta d \\
\beta^{(0)} \text{ is the initial guess } \\
\beta^{(1)} \text{ is a guess after the first correction } \\
\eta \text{ is the learning rate } \\
d \text{ is the descent direction } \\
\end{align*}
$$
   
     - gradient descent: find maximun, d=-1
     - gradient ascent: find minimun, d=1

