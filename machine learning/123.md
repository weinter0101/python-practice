# Marchine Learning

## 1. k-NN regression

```python
import numpy as np
import matplotlib.pyplot as plt

# create data
np.random.seed(20)
x = np.random.uniform(0, 10, 18)
y = np.random.uniform(0, 10, 18)

# create scatter plot
plt.figure(figsize=(8, 6))
plt.scatter(x, y, c='blue', label='$Random\; Points$')
plt.title('$Scatter\; Plot\; with\; 18\; Random\; Points$')
plt.xlabel('$X$')
plt.ylabel('$Y$')
plt.legend()
plt.show()
```
![scatter](https://github.com/weinter0101/python-practice/blob/main/machine%20learning/graph/Figure1.1.png)


### k=n
- k=1
```python
k = 1
knnRegressor = KNeighborsRegressor(k)
knnRegressor.fit(x, y)

T = np.linspace(0, 10, 500)[:, np.newaxis] 
knnLine = knnRegressor.predict(T)

plt.figure(figsize=(10, 6))
plt.scatter(x, y, color='blue', label='$Data\; Points$')
plt.plot(T, knnLine, color='red', label=f'$k={k}\; Regression$')
plt.title('$k-NN\; Regression\; with\; k=3$')
plt.xlabel('$X$')
plt.ylabel('$Y$')
plt.legend()
plt.show()
```
![scatter](https://github.com/weinter0101/python-practice/blob/main/machine%20learning/graph/Figure1.2.png)

- k=2, 3, 5, 18
     [code](https://gist.github.com/7e0deab4e3c6ca9323e3c195fc77b71f.git)
![scatter](https://github.com/weinter0101/python-practice/blob/main/machine%20learning/graph/Figure1.3.png)

1. k值過小：overfitting, 無法對未見過的數據進行準確預測。
2. k值過大：underfitting, 由於過度平滑所以對訓練數據和新數據都不能進行有效的預測。

## 1. Linear Regression Model
- find a linear function to fit oberseve data
- Least Squares (LS) estimation:
     - Fitting criterion: sum of square distance between actual y and predicted y in the traning set
     - Find a function that minimizes this critersoin.
- general form of linear function:
     
\begin{align*}
\mathbf{f}(\mathbf{x}; \boldsymbol{\beta}) &= \beta_0 + \beta_1 x_1 + \beta_2 x_2 + \ldots + \beta_k x_k \quad (x_0 = 1) \\
&= \sum_{j=0}^{k} \beta_j x_j \\
&= \mathbf{x}^{\top} \boldsymbol{\beta},
\end{align*}

- define a loss function(fitting criterion):


